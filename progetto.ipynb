{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmIYaBRYVe79"
   },
   "source": [
    "# Create Data Set by TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYCmg2gIYhka"
   },
   "source": [
    "First, let's define a function to preprocess frases using stemming and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "zrCkQMC9Ythc",
    "outputId": "5d8f0a7c-fb11-49b1-8a7e-249d569fa326"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "porter_stemmer = PorterStemmer()\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "#binary search to improve efficency\n",
    "def binary_search(e,l,inizio,fine):\n",
    "  if inizio>fine or e<l[inizio] or e>l[fine-1]:\n",
    "    return False\n",
    "  else:\n",
    "    mezzo=int((inizio+fine)/2)\n",
    "    m=l[mezzo]\n",
    "    if e==m:\n",
    "      return True\n",
    "    elif e>m:\n",
    "      return binary_search(e,l,mezzo,fine)\n",
    "    else:\n",
    "      return binary_search(e,l,inizio,mezzo)\n",
    "\n",
    "#removing symbols and adding space after them\n",
    "def prepreprocess(frase):\n",
    "  s=''\n",
    "  syms=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','è','é','à','ò','ì','ù']\n",
    "  for char in frase:\n",
    "    if char not in syms:\n",
    "      s+=' '\n",
    "    else:\n",
    "      s+=char\n",
    "  return s\n",
    "\n",
    "#preprocess using prepreprocess fase and stemming and removing stopwords\n",
    "def preprocess(frase,en_words):\n",
    "  l=[]\n",
    "  for word in prepreprocess(frase.lower()).split(' '):\n",
    "    w=word.lower()\n",
    "    if w not in en_stops and len(w)>2 and binary_search(w,en_words,0,len(en_words)):\n",
    "        l.append(porter_stemmer.stem(w))\n",
    "  return repr(' '.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOboDiejGra4"
   },
   "source": [
    "Let's load now the data frame, saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7z3J9YbMAQUg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('dataset/reviews_Video_Games_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZqSrHJLv_6W"
   },
   "source": [
    "Now we can preprocess the whole data set using previous function. Preprocess is calculated on the concatenation of summary and review text and result is added in a new column ('text')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nE-MNzrZv8of"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# en_words=sorted(open('/content/drive/My Drive/uni/ml/progetto/en_words.txt', 'r').read().lower().split('\\n'))\n",
    "\n",
    "# df['text']=df['summary']+' '+df['reviewText']\n",
    "# df['text']=df['text'].apply(lambda x:preprocess(str(x.lower()),en_words))\n",
    "\n",
    "en_words=sorted(open('/content/drive/My Drive/uni/ml/progetto/en_words.txt', 'r').read().lower().split('\\n'))\n",
    "reader = csv.reader(open('/content/drive/My Drive/uni/ml/progetto/reviews_Video_Games_5.csv','rt'))\n",
    "\n",
    "new_df=[]\n",
    "tags=next(reader)\n",
    "for row in reader:\n",
    "  row[9]=preprocess(str(row[6]+' '+row[4]),en_words)\n",
    "  new_df.append(row)\n",
    "\n",
    "df=pd.DataFrame(new_df, columns=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbryUMF5uyU4"
   },
   "source": [
    "This is only for the first time. The result of previous code is a new data frame that contains a new field where there is the result of preprocessing. In order to don't repeat it another time (preprocess is too slow) we saved it on a new csv file from which we can start the rest of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7o25ibg3FCCb"
   },
   "outputs": [],
   "source": [
    "df.to_csv('/content/drive/My Drive/uni/ml/progetto/reviews_Video_Games_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMLO7Ib_rjvG"
   },
   "source": [
    "We now need to create the vocabulary and start the counting process. We can use the CountVectorizer to create a vocabulary from all the text in our df['text'] followed by the counts of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "X0AyYCdkro9N",
    "outputId": "d7596158-2f80-43c6-b655-512426589cb4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "  \"\"\"load stop words \"\"\"    \n",
    "  with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = f.readlines()\n",
    "    stop_set = set(m.strip() for m in stopwords)\n",
    "  return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words('/content/drive/My Drive/uni/ml/progetto/stopwords.txt')\n",
    "\n",
    "#get the text column \n",
    "docs=df['text'].tolist()\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 99,999999% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEkWNADisUR1"
   },
   "source": [
    "Its now time to compute the IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EX35FtVRJiMe"
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf = TfidfVectorizer()\n",
    "# features = tfidf.fit_transform(df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SqMZcerIsU7H",
    "outputId": "6480fd40-6b09-4107-f774-550d50284586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1-48ZioscT6"
   },
   "source": [
    "We are now ready to compute TF-IDF and then extract top keywords from the TF-IDF vectors. First, let's separate data set from test set to extract top keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXYTi3iNlokb"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "ds=[]\n",
    "ts=[]\n",
    "\n",
    "reader = csv.reader(open('/content/drive/My Drive/uni/ml/progetto/reviews_Video_Games_5.csv','rt'))\n",
    "\n",
    "tags=next(reader)\n",
    "for row in reader:\n",
    "  if random.randint(1,40)>1:\n",
    "    ds.append(row)\n",
    "  else:\n",
    "    ts.append(row)\n",
    "    \n",
    "dataset=pd.DataFrame(ds, columns=tags)\n",
    "testset=pd.DataFrame(ts, columns=tags)\n",
    "\n",
    "# get test docs into a list\n",
    "docs_test=testset['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yuGRTFJJqdkx"
   },
   "source": [
    "The next step is to compute the tf-idf value for a given document in our test set that generates a vector of tf-idf scores. Next, we sort the words in the vector in descending order of tf-idf values and then iterate over to extract the top-n keywords.We are extracting keywords for the first document in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuKnsUqpqey1"
   },
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "  tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "  return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "  \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "  #use only topn items from vector\n",
    "  sorted_items = sorted_items[:topn] \n",
    "  score_vals = []\n",
    "  feature_vals = []\n",
    "  # word index and corresponding tf-idf score\n",
    "  for idx, score in sorted_items:\n",
    "    #keep track of feature name and its corresponding score\n",
    "    score_vals.append(round(score, 3))\n",
    "    feature_vals.append(feature_names[idx])\n",
    "  #create a tuples of feature,score\n",
    "  #results = zip(feature_vals,score_vals)\n",
    "  results= {}\n",
    "  for idx in range(len(feature_vals)):\n",
    "    results[feature_vals[idx]]=score_vals[idx]    \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vu4ua92gaOiF"
   },
   "outputs": [],
   "source": [
    "# dizionario={}\n",
    "# for e in cv.get_feature_names():\n",
    "#   dizionario[e]=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAA5J6p3ElNh"
   },
   "source": [
    "Now we are ready to compute tf-idf values for words in each row. We create a list which each element is a dictionary that contains the word as a key and the values is the tf-idf value of that word. Each dictionary is reffered to a single review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-foFZHVKcN0u"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "i=0\n",
    "dicts=[]\n",
    "for doc in df['text'].tolist():\n",
    "  if(i%10000==0):\n",
    "    print(i)\n",
    "  #generate tf-idf for the given document\n",
    "  tf_idf_vector=tfidf_transformer.transform(cv.transform([doc])) \n",
    "  #sort the tf-idf vectors by descending order of scores\n",
    "  sorted_items=sort_coo(tf_idf_vector.tocoo()) \n",
    "  #extract all items from document\n",
    "  keywords=extract_topn_from_vector(feature_names,sorted_items,len(doc))\n",
    "  d={}\n",
    "  for e in cv.get_feature_names():\n",
    "    if e in keywords:\n",
    "      d[e]=keywords[e]\n",
    "  dicts.append(d)\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HVVLZmD2GU3A",
    "outputId": "eb503a0a-d507-42f3-f592-446653242806"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35681"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rS3ZpBPUGYn7"
   },
   "source": [
    "In order to don't create a data frame with 35681 attributes, we take only the top 100 words most used in all reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gA8TnvuFJEQE"
   },
   "outputs": [],
   "source": [
    "def min_list(l):\n",
    "  m=l[0]\n",
    "  for e in l:\n",
    "    if e<m:\n",
    "      m=e\n",
    "  return m\n",
    "\n",
    "def remove_value(d,v):\n",
    "  keys=d.keys()\n",
    "  for e in keys:\n",
    "    if d[e]==v:\n",
    "      del d[e]\n",
    "      break\n",
    "      \n",
    "keycount={}\n",
    "for e in cv.get_feature_names():\n",
    "  keycount[e]=0\n",
    "for d in dicts:\n",
    "  keys=d.keys()\n",
    "  for k in keys:\n",
    "    keycount[k]+=1\n",
    "\n",
    "minv=0\n",
    "top100={}\n",
    "for k in keycount.keys():\n",
    "  if len(top100)<100:\n",
    "    top100[k]=keycount[k]\n",
    "    minv=min_list(list(top100.values()))\n",
    "  else:\n",
    "    if keycount[k]>minv:\n",
    "      remove_value(top100,minv)\n",
    "      top100[k]=keycount[k]\n",
    "      minv=min_list(list(top100.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U64lx-zeqhpw"
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # tfidf = TfidfVectorizer()\n",
    "# # features = tfidf.fit_transform(df['text'].tolist())\n",
    "\n",
    "# # you only needs to do this once, this is a mapping of index to \n",
    "# feature_names=cv.get_feature_names()\n",
    " \n",
    "# # get the document that we want to extract keywords from\n",
    "# r=random.randint(0,len(ts))\n",
    "# doc=docs_test[r]\n",
    " \n",
    "# #generate tf-idf for the given document\n",
    "# tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    " \n",
    "# #sort the tf-idf vectors by descending order of scores\n",
    "# sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "# #extract only the top n; n here is 10\n",
    "# keywords=extract_topn_from_vector(feature_names,sorted_items,len(doc))\n",
    " \n",
    "# # now print the results\n",
    "# print(\"\\n=====Doc=====\")\n",
    "# print(r,len(doc.split(' ')),doc)\n",
    "# print(\"\\n===Keywords===\")\n",
    "# for k in keywords:\n",
    "#   print(k,keywords[k])\n",
    "# print(len(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f18uh16eHoaZ"
   },
   "source": [
    "We are now ready to create the data frame which: first column is the review text, the second one is the utility (calculated as the ratio between usefull rates and total rate and is 1 if this ratio is more equal than 0.7, 0 otherwise), the other columns are tf-idf value of each word in the columns in each review (if the word isn't in the review text value is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiBfAwOi4-6h"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "reader = csv.reader(open('/content/drive/My Drive/uni/ml/progetto/reviews_Video_Games_5.csv','rt'))\n",
    "\n",
    "dataframe=[]\n",
    "tags=['text','utility']\n",
    "\n",
    "for e in list(top100.keys()):\n",
    "  tags.append(e)\n",
    "\n",
    "def utility(a,b):\n",
    "  if b==0.0: return 0.0\n",
    "  return a/b\n",
    "\n",
    "i=0\n",
    "next(reader)\n",
    "for row in reader:\n",
    "  r=[]\n",
    "  r.append(row[9])\n",
    "  u=utility(float(eval(row[3])[0]),float(eval(row[3])[1]))\n",
    "  if u>=0.7:\n",
    "    r.append(1)\n",
    "  else:\n",
    "    r.append(0)\n",
    "  for e in top100.keys():\n",
    "    if e in dicts[i].keys():\n",
    "      r.append(dicts[i][e])\n",
    "    else:\n",
    "      r.append(0)\n",
    "  dataframe.append(r)\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Tl107CJJRne"
   },
   "source": [
    "This is only for the first time we create the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksSdN9Mk6qGR"
   },
   "outputs": [],
   "source": [
    "DF=pd.DataFrame(dataframe, columns=tags)\n",
    "DF.to_csv('/content/drive/My Drive/uni/ml/progetto/reviews_Video_Games_5-data_frame.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwOdxGJzJbl_"
   },
   "source": [
    "# Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "7IvcBOreJi2E",
    "outputId": "21e4ae60-ab06-41d6-9653-7481cf44828a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>utility</th>\n",
       "      <th>actual</th>\n",
       "      <th>anoth</th>\n",
       "      <th>awesom</th>\n",
       "      <th>back</th>\n",
       "      <th>bad</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>...</th>\n",
       "      <th>time</th>\n",
       "      <th>tri</th>\n",
       "      <th>version</th>\n",
       "      <th>way</th>\n",
       "      <th>weapon</th>\n",
       "      <th>well</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'pay unlock content think instal game struggl ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'good ralli game like ralli car get game fun o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'wrong key shipment receiv book instead game s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'awesom game crash frequent got version instea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'dirt dirt okay game start play game laptop bo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  utility  actual  anoth  \\\n",
       "0  'pay unlock content think instal game struggl ...        0     0.0  0.058   \n",
       "1  'good ralli game like ralli car get game fun o...        0     0.0  0.000   \n",
       "2  'wrong key shipment receiv book instead game s...        0     0.0  0.000   \n",
       "3  'awesom game crash frequent got version instea...        1     0.0  0.000   \n",
       "4  'dirt dirt okay game start play game laptop bo...        1     0.0  0.000   \n",
       "\n",
       "   awesom  back  bad  best  big  bit  ...   time   tri  version  way  weapon  \\\n",
       "0    0.00   0.0  0.0  0.00  0.0  0.0  ...  0.000  0.00    0.000  0.0     0.0   \n",
       "1    0.00   0.0  0.0  0.08  0.0  0.0  ...  0.000  0.00    0.000  0.0     0.0   \n",
       "2    0.00   0.0  0.0  0.00  0.0  0.0  ...  0.000  0.00    0.000  0.0     0.0   \n",
       "3    0.03   0.0  0.0  0.00  0.0  0.0  ...  0.073  0.05    0.152  0.0     0.0   \n",
       "4    0.00   0.0  0.0  0.00  0.0  0.0  ...  0.000  0.00    0.000  0.0     0.0   \n",
       "\n",
       "    well   work  world  worth   year  \n",
       "0  0.000  0.000    0.0  0.000  0.000  \n",
       "1  0.000  0.084    0.0  0.000  0.000  \n",
       "2  0.000  0.000    0.0  0.000  0.000  \n",
       "3  0.021  0.024    0.0  0.028  0.054  \n",
       "4  0.000  0.000    0.0  0.000  0.000  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DF=pd.read_csv('dataset/reviews_Video_Games_5-data_frame.csv')\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "HEKjxZJ2GPMf",
    "outputId": "6550e518-bdfa-44cb-ff87-3d09c9b9688a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>utility</th>\n",
       "      <th>actual</th>\n",
       "      <th>anoth</th>\n",
       "      <th>awesom</th>\n",
       "      <th>back</th>\n",
       "      <th>bad</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>...</th>\n",
       "      <th>time</th>\n",
       "      <th>tri</th>\n",
       "      <th>version</th>\n",
       "      <th>way</th>\n",
       "      <th>weapon</th>\n",
       "      <th>well</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231775</th>\n",
       "      <td>'rate system seller funni peopl rate seller ri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231776</th>\n",
       "      <td>'get bundl includ extra wheel control delux ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231777</th>\n",
       "      <td>'fake bundl packag red show steer wheel retail...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231778</th>\n",
       "      <td>'look like gouger get packag mine arriv box red'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231779</th>\n",
       "      <td>'buy look european version real retail store t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  utility  actual  \\\n",
       "231775  'rate system seller funni peopl rate seller ri...        0   0.000   \n",
       "231776  'get bundl includ extra wheel control delux ma...        1   0.000   \n",
       "231777  'fake bundl packag red show steer wheel retail...        1   0.000   \n",
       "231778   'look like gouger get packag mine arriv box red'        1   0.000   \n",
       "231779  'buy look european version real retail store t...        1   0.088   \n",
       "\n",
       "        anoth  awesom  back  bad   best  big  bit  ...  time   tri  version  \\\n",
       "231775    0.0     0.0   0.0  0.0  0.000  0.0  0.0  ...   0.0  0.00    0.000   \n",
       "231776    0.0     0.0   0.0  0.0  0.000  0.0  0.0  ...   0.0  0.00    0.000   \n",
       "231777    0.0     0.0   0.0  0.0  0.000  0.0  0.0  ...   0.0  0.00    0.000   \n",
       "231778    0.0     0.0   0.0  0.0  0.000  0.0  0.0  ...   0.0  0.00    0.000   \n",
       "231779    0.0     0.0   0.0  0.0  0.073  0.0  0.0  ...   0.0  0.08    0.194   \n",
       "\n",
       "        way  weapon  well  work  world  worth  year  \n",
       "231775  0.0     0.0   0.0   0.0    0.0    0.0   0.0  \n",
       "231776  0.0     0.0   0.0   0.0    0.0    0.0   0.0  \n",
       "231777  0.0     0.0   0.0   0.0    0.0    0.0   0.0  \n",
       "231778  0.0     0.0   0.0   0.0    0.0    0.0   0.0  \n",
       "231779  0.0     0.0   0.0   0.0    0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0RQwhIgMY5q"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "reader=csv.reader(open('dataset/reviews_Video_Games_5-data_frame.csv','rt'))\n",
    "\n",
    "next(reader)\n",
    "for row in reader:\n",
    "  target.append(row[1])\n",
    "  l=[]\n",
    "  for e in row[2:]:\n",
    "    l.append(float(e))\n",
    "  data.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgPWqBHXMJeF"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb=GaussianNB()\n",
    "pred=gnb.fit(data,target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HoyOfyl-ObYg",
    "outputId": "3ef68ab8-3625-4e9e-c14c-f7f337a3afea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 231780 points : 100682\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (len(data),(target != pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZybHGT-5XGXS"
   },
   "outputs": [],
   "source": [
    "def compute_tfidf(doc,cv,tfidf_transformer,attributes):\n",
    "  #generate tf-idf for the given document\n",
    "  tf_idf_vector=tfidf_transformer.transform(cv.transform([doc])) \n",
    "  #sort the tf-idf vectors by descending order of scores\n",
    "  sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "  #extract all items from document\n",
    "  keywords=extract_topn_from_vector(feature_names,sorted_items,len(doc))\n",
    "  dict_doc={}\n",
    "  for e in attributes:\n",
    "    if e in keywords:\n",
    "      dict_doc[e]=keywords[e]\n",
    "    else:\n",
    "      dict_doc[e]=0\n",
    "  return dict_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "  \"\"\"load stop words \"\"\"    \n",
    "  with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    stopwords = f.readlines()\n",
    "    stop_set = set(m.strip() for m in stopwords)\n",
    "  return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words('stopwords.txt')\n",
    "\n",
    "#get the text column \n",
    "docs=df['text'].tolist()\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 99,999999% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qy5q0I9ATN0a"
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f63cd9c941bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# feature_names=cv.get_feature_names()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdict_doc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdata_doc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e5a580549d08>\u001b[0m in \u001b[0;36mcompute_tfidf\u001b[1;34m(doc, cv, tfidf_transformer, attributes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattributes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;31m#generate tf-idf for the given document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mtf_idf_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[1;31m#sort the tf-idf vectors by descending order of scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0msorted_items\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_coo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "summary='Great game of all the time!!!'\n",
    "reviewText=\"I've never played this game first, but since the initial install I've fallen in love <3 !!! It's amazing because shooting mode is very easy and level of entertainment is fantastic! I'll play for hour and hour so long!!! Very recommended!!!!\"\n",
    "\n",
    "# summary='hahah'\n",
    "# reviewText=\"I like obama because he loves me so much and other people wanna play this game\"\n",
    "\n",
    "en_words=sorted(open('en_words.txt', 'r').read().lower().split('\\n'))\n",
    "doc=preprocess(str(summary+' '+reviewText),en_words)\n",
    "\n",
    "cv=CountVectorizer(stop_words=stopwords)\n",
    "# feature_names=cv.get_feature_names()\n",
    "\n",
    "dict_doc=compute_tfidf(doc,cv,tfidf_transformer,list(DF.columns[2:]))\n",
    "\n",
    "data_doc=[]\n",
    "for v in dict_doc.values():\n",
    "  data_doc.append(float(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Lkt1bQ6MSxRm",
    "outputId": "2c982a65-1592-4c06-9772-ef4d3270ca1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gnb_pf=GaussianNB()\n",
    "gnb_pf.partial_fit(data,target,np.unique(target))\n",
    "print(gnb_pf.predict([data_doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVxtW4uZDQTw"
   },
   "source": [
    "# Classification using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lVD0smojDVBa",
    "outputId": "2841d048-63d9-4f27-b924-86015fb585e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pgwCq1cD3r8"
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2rKrBn2D50b"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv('dataset/reviews_Video_Games_5-data_frame.csv')#, header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,2:].astype(float)\n",
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "buuauM-THPXF",
    "outputId": "d9cfd8a0-7735-4dea-c7de-bb498c8aef86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.058, 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       ...,\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.088, 0.   , 0.   , ..., 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cmx2pCudGKGD"
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjWJG58kGjo7"
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=100, activation='relu'))\n",
    "\tmodel.add(Dense(2, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLS_pCmQGlXb"
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfPKFOC0GnQq"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "6UU280KgGozJ",
    "outputId": "a245b89c-c8b8-45b8-a9eb-bcd38ca5def2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Baseline: 70.74% (0.21%)\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "u0KhbQucWAc4"
   ],
   "name": "progetto.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
